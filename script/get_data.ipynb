{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7748382-cef5-481d-8d49-af2e8dcfe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b88d681-dafd-4cc0-999b-3a3f3b968af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_names(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Извлекаем все ссылки на файлы\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # Фильтруем ссылки, оставляя только те, которые оканчиваются на _inter_30.hic\n",
    "    prefixes = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href.endswith('_inter_30.hic'):\n",
    "            # Извлекаем префикс\n",
    "            prefix = href.split('_inter_30.hic')[0]\n",
    "            prefixes.append(prefix)\n",
    "    return prefixes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5ba1114-d409-4619-8a1f-5f3d25cac295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(sample_names, base_url, file_type, out_dir):\n",
    "    # Определяем шаблон файла и соответствующую папку для сохранения\n",
    "    if file_type == 'hic':\n",
    "        file_pattern = 'inter_30.hic'\n",
    "        local_dir = os.path.join(out_dir, 'hic')\n",
    "    elif file_type == 'merged_nodups':\n",
    "        file_pattern = '_merged_nodups.txt.gz'\n",
    "        local_dir = os.path.join(out_dir, 'merged_nodups')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file type. Use 'hic' or 'merged_nodups'.\")\n",
    "    \n",
    "    # Создаем локальную папку, если она не существует\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    for sample in sample_names:\n",
    "        sample_url = f\"{base_url}/{sample}/\"\n",
    "    \n",
    "        try:\n",
    "            with urllib.request.urlopen(sample_url) as response:\n",
    "                html = response.read().decode('utf-8')\n",
    "                lines = html.splitlines()\n",
    "    \n",
    "                for line in lines:\n",
    "                    if '<a href=\"' in line:\n",
    "                        dir_name = line.split('<a href=\"')[1].split('/')[0]\n",
    "                        if not dir_name.startswith('?') and not dir_name.startswith('..'):\n",
    "                            subdir_url = f\"{sample_url}{dir_name}/\"\n",
    "                            suffix = re.split('[_-]', dir_name)[-1]\n",
    "                        \n",
    "                            try:\n",
    "                                with urllib.request.urlopen(subdir_url) as sub_response:\n",
    "                                    sub_html = sub_response.read().decode('utf-8')\n",
    "                                    sub_lines = sub_html.splitlines()\n",
    "                                    \n",
    "                                    for sub_line in sub_lines:\n",
    "                                        if file_pattern in sub_line:\n",
    "                                            file_name = sub_line.split('<a href=\"')[1].split('\"')[0]\n",
    "                                            file_url = f\"{subdir_url}{file_name}\"\n",
    "                                            file_name_new = '_'.join(file_name.split('_')[:2]) + '_' + suffix + '_' + '_'.join(file_name.split('_')[2:])\n",
    "                                            \n",
    "                                            if not os.path.exists(os.path.join(local_dir, file_name_new)):\n",
    "                                                local_filepath = os.path.join(local_dir, file_name_new)\n",
    "                                                    \n",
    "                                                # print(f\"Downloading {file_name} to {local_filepath}...\")\n",
    "                                                urllib.request.urlretrieve(file_url, local_filepath)\n",
    "                                                print(f\"Downloaded {file_name_new} successfully!\")\n",
    "                                            else:\n",
    "                                                print(f\"{file_name} exists\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error accessing subdirectory {dir_name} in {sample}: {e}\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing sample directory {sample}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873621b8-01dd-45be-979b-d0b50aade4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://genedev.bionet.nsc.ru/ftp/by_Project/LowInHIC_for_BCA/Samples\"\n",
    "out_dir = \"/media/eternus1/nfs/projects/dpanchenko/test_embrio/data/\"\n",
    "\n",
    "url = \"https://genedev.bionet.nsc.ru/ftp/by_Project/LowInHIC_for_BCA/Translocations_Andrei/\"\n",
    "prefixes = get_sample_names(url)\n",
    "\n",
    "download_files(prefixes, base_url, 'hic', out_dir)\n",
    "download_files(prefixes, base_url, 'merged_nodups', out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e372f-935f-4cd6-9ca5-7f21df52cccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52471a-38a2-4793-ab53-c4130ccec687",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download genome size files (optional)\n",
    "# These files store in data/genome dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cef99-71e1-43c2-a115-88c9ac8b9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_filepath = '/media/eternus1/nfs/projects/dpanchenko/test_embrio/data/genome/'\n",
    "t2t_genome = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/914/755/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_assembly_report.txt'\n",
    "hg19_genome = 'https://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/referenceSequences/male.hg19.chrom.sizes'\n",
    "\n",
    "urllib.request.urlretrieve(t2t_genome, os.path.join(local_filepath, 'GCF_009914755.1_T2T-CHM13v2.0_assembly_report.txt'))\n",
    "urllib.request.urlretrieve(hg19_genome, os.path.join(local_filepath, 'male.hg19.chrom.sizes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092da3a8-f574-41e7-8aa9-099edbdeef8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
